id: load_orders_bigquery
namespace: shiny_rocks
description: |
  When data are generated upstream, this flow ingest the `orders` data into Google Cloud Storage and BigQuery.

labels:
  tag: load

inputs:
  - id: orders_data
    type: URI

  - id: order_date
    type: DATE

tasks:

  - id: extract
    type: io.kestra.plugin.gcp.gcs.Upload
    serviceAccount: '{{ secret("GCP_CREDS") }}'
    from: "{{ inputs.orders_data }}"
    to: gs://shiny_rocks_project/app_log/orders/{{ inputs.order_date }}/orders.csv

  - id: load
    type: io.kestra.plugin.gcp.bigquery.LoadFromGcs
    from: 
      - "{{ outputs.extract.uri }}"
    projectId: "kestra-sandbox"
    destinationTable: "shiny_rocks.orders"
    serviceAccount: '{{ secret("GCP_CREDS") }}'
    format: CSV
    autodetect: true
    csvOptions:
      fieldDelimiter: ","
    timePartitioningField: "order_date"

triggers:

  - id: get_data
    type: io.kestra.plugin.core.trigger.Flow
    inputs:
      orders_data: "{{ outputs.python.outputFiles['orders.csv'] }}"
      order_date: "{{ outputs.run_date.value }}"
    conditions:
      - type: io.kestra.plugin.core.condition.ExecutionFlowCondition
        namespace: shiny_rocks
        flowId: produce_data
      - type: io.kestra.plugin.core.condition.ExecutionStatusCondition
        in:
          - SUCCESS